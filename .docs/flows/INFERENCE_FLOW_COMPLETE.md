# Inference Request Flow: Complete Roundtrip

**Flow:** Client â†’ Queen â†’ Scheduler â†’ Worker â†’ Tokens â†’ Queen â†’ Client  
**Date:** November 2, 2025  
**Status:** âœ… COMPLETE

---

## Overview

This document traces the complete inference request flow from when a client submits a request to Queen, through worker selection and execution, to streaming tokens back to the client via SSE.

**Example Command:**
```bash
rbee-keeper infer --model tinyllama --prompt "Hello, world!"
```

---

## Complete Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLIENT (rbee-keeper)                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Build Operation::Infer                                   â”‚
â”‚    â”œâ”€â†’ model: "tinyllama"                                  â”‚
â”‚    â”œâ”€â†’ prompt: "Hello, world!"                             â”‚
â”‚    â”œâ”€â†’ max_tokens: 100                                     â”‚
â”‚    â””â”€â†’ temperature: 0.7                                    â”‚
â”‚    â†“                                                        â”‚
â”‚ 2. POST /v1/jobs to Queen                                  â”‚
â”‚    â†“                                                        â”‚
â”‚ 3. Receive JobResponse                                      â”‚
â”‚    â”œâ”€â†’ job_id: "job_abc123"                                â”‚
â”‚    â””â”€â†’ sse_url: "/v1/jobs/job_abc123/stream"              â”‚
â”‚    â†“                                                        â”‚
â”‚ 4. GET /v1/jobs/job_abc123/stream                          â”‚
â”‚    â†“                                                        â”‚
â”‚ 5. Receive SSE events                                       â”‚
â”‚    â”œâ”€â†’ data: {"action":"infer_schedule",...}               â”‚
â”‚    â”œâ”€â†’ data: {"action":"infer_token","formatted":"Hello"}  â”‚
â”‚    â”œâ”€â†’ data: {"action":"infer_token","formatted":"!"}      â”‚
â”‚    â””â”€â†’ data: [DONE]                                        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUEEN (Port 7833)                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. POST /v1/jobs receives Operation::Infer                 â”‚
â”‚    â†“                                                        â”‚
â”‚ 2. create_job() in job_router.rs                           â”‚
â”‚    â”œâ”€â†’ Generate job_id                                     â”‚
â”‚    â”œâ”€â†’ Create SSE channel                                  â”‚
â”‚    â”œâ”€â†’ Store payload in registry                           â”‚
â”‚    â””â”€â†’ Return JobResponse                                  â”‚
â”‚    â†“                                                        â”‚
â”‚ 3. Client connects to SSE stream                           â”‚
â”‚    â†“ triggers execute_job()                                â”‚
â”‚    â†“                                                        â”‚
â”‚ 4. Parse Operation::Infer from payload                     â”‚
â”‚    â†“                                                        â”‚
â”‚ 5. Create JobRequest for scheduler                         â”‚
â”‚    â”œâ”€â†’ job_id, model, prompt                               â”‚
â”‚    â””â”€â†’ max_tokens, temperature, top_p                      â”‚
â”‚    â†“                                                        â”‚
â”‚ 6. scheduler.schedule(job_request)                         â”‚
â”‚    â”œâ”€â†’ Find workers with model                             â”‚
â”‚    â”œâ”€â†’ Filter by availability (gpu_util == 0)              â”‚
â”‚    â””â”€â†’ Return ScheduleResult                               â”‚
â”‚    â†“                                                        â”‚
â”‚ 7. scheduler.execute_job()                                 â”‚
â”‚    â”œâ”€â†’ POST to worker /v1/jobs                             â”‚
â”‚    â”œâ”€â†’ Connect to worker SSE stream                        â”‚
â”‚    â””â”€â†’ Forward tokens to Queen SSE channel                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WORKER (Port 9000+)                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. POST /v1/jobs receives Operation::Infer                 â”‚
â”‚    â†“                                                        â”‚
â”‚ 2. create_job() in worker job_router.rs                    â”‚
â”‚    â”œâ”€â†’ Generate worker job_id                              â”‚
â”‚    â”œâ”€â†’ Create MPSC channel for tokens                      â”‚
â”‚    â”œâ”€â†’ Create GenerationRequest                            â”‚
â”‚    â””â”€â†’ Add to request queue                                â”‚
â”‚    â†“                                                        â”‚
â”‚ 3. Generation engine (spawn_blocking thread)               â”‚
â”‚    â”œâ”€â†’ Pull request from queue                             â”‚
â”‚    â”œâ”€â†’ Lock backend (exclusive access)                     â”‚
â”‚    â”œâ”€â†’ Tokenize prompt                                     â”‚
â”‚    â”œâ”€â†’ Reset KV cache                                      â”‚
â”‚    â””â”€â†’ Generate tokens one by one                          â”‚
â”‚    â†“                                                        â”‚
â”‚ 4. For each token:                                          â”‚
â”‚    â”œâ”€â†’ Send through MPSC channel                           â”‚
â”‚    â””â”€â†’ Release lock briefly (backpressure)                 â”‚
â”‚    â†“                                                        â”‚
â”‚ 5. SSE handler (async task)                                â”‚
â”‚    â”œâ”€â†’ Receive tokens from channel                         â”‚
â”‚    â”œâ”€â†’ Format as SSE events                                â”‚
â”‚    â””â”€â†’ Stream to Queen                                     â”‚
â”‚    â†“                                                        â”‚
â”‚ 6. Send [DONE] marker                                       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Step-by-Step Breakdown

### Step 1: Client Submits Request

**File:** `bin/00_rbee_keeper/src/handlers/infer.rs`

```rust
pub async fn handle_infer(
    model: String,
    prompt: String,
    max_tokens: Option<u32>,
    temperature: Option<f32>,
    queen_url: &str,
) -> Result<()> {
    // Build Operation::Infer
    let operation = Operation::Infer(InferRequest {
        hive_id: "localhost".to_string(),
        model,
        prompt,
        max_tokens,
        temperature,
        top_p: None,
        top_k: None,
        device: None,
        worker_id: None,
        stream: true,
    });
    
    // Submit to queen
    let client = JobClient::new(queen_url);
    client.submit_and_stream(operation, |line| {
        println!("{}", line);
        Ok(())
    }).await?;
    
    Ok(())
}
```

**Purpose:** Build and submit inference request

---

### Step 2: Queen Creates Job

**File:** `bin/10_queen_rbee/src/job_router.rs`

```rust
pub async fn create_job(
    state: JobState,
    payload: serde_json::Value,
) -> Result<JobResponse> {
    // Generate unique job ID
    let job_id = state.registry.create_job();
    
    // Create job-specific SSE channel
    observability_narration_core::sse_sink::create_job_channel(job_id.clone(), 1000);
    
    // Store payload for later execution
    state.registry.store_payload(&job_id, payload);
    
    // Return job info to client
    Ok(JobResponse {
        job_id: job_id.clone(),
        sse_url: format!("/v1/jobs/{}/stream", job_id),
    })
}
```

**Location:** Lines 51-67  
**Purpose:** Create job and SSE channel

**Narration Events:**
- `job_create` â€” Job created, waiting for client connection

---

### Step 3: Client Connects to SSE

**File:** `bin/10_queen_rbee/src/http/jobs.rs`

```rust
pub async fn handle_stream_job(
    Path(job_id): Path<String>,
    State(state): State<SchedulerState>,
) -> Sse<impl Stream<Item = Result<Event, Infallible>>> {
    // Take the receiver (single-use)
    let sse_rx_opt = sse_sink::take_job_receiver(&job_id);
    
    // Trigger job execution
    let _token_stream = crate::job_router::execute_job(job_id.clone(), state.into()).await;
    
    // Stream events to client
    let combined_stream = async_stream::stream! {
        let Some(mut sse_rx) = sse_rx_opt else {
            yield Ok(Event::default().data("ERROR: Job channel not found"));
            return;
        };
        
        loop {
            match sse_rx.recv().await {
                Some(event) => {
                    let json = serde_json::to_string(&event).unwrap();
                    yield Ok(Event::default().data(&json));
                }
                None => {
                    yield Ok(Event::default().data("[DONE]"));
                    break;
                }
            }
        }
    };
    
    Sse::new(combined_stream)
}
```

**Location:** Lines 109-175  
**Purpose:** Stream events to client

---

### Step 4: Queen Parses and Schedules

**File:** `bin/10_queen_rbee/src/job_router.rs`

```rust
async fn route_operation(
    job_id: String,
    payload: serde_json::Value,
    registry: Arc<JobRegistry<String>>,
    hive_registry: Arc<TelemetryRegistry>,
) -> Result<()> {
    // Parse operation
    let operation: Operation = serde_json::from_value(payload)?;
    
    n!("route_job", "Executing operation: {}", operation.name());
    
    // Match operation type
    match operation {
        Operation::Infer(req) => {
            // Create job request
            let job_request = JobRequest {
                job_id: job_id.clone(),
                model: req.model.clone(),
                prompt: req.prompt.clone(),
                max_tokens: req.max_tokens,
                temperature: req.temperature,
                top_p: req.top_p,
                top_k: req.top_k,
            };
            
            // Create scheduler
            let scheduler = SimpleScheduler::new(hive_registry.clone());
            
            // Schedule to worker
            n!("infer_schedule", "ğŸ“‹ Scheduling inference to worker");
            let schedule_result = scheduler.schedule(job_request.clone()).await?;
            
            n!("infer_worker", "âœ… Scheduled to worker: {}", schedule_result.worker_id);
            
            // Execute on worker with streaming
            scheduler.execute_job(schedule_result, job_request, |line| {
                n!("infer_token", "{}", line);
                Ok(())
            }).await?;
            
            n!("infer_complete", "âœ… Inference complete");
        }
        _ => {}
    }
    
    Ok(())
}
```

**Location:** Lines 92-197  
**Purpose:** Parse operation and schedule to worker

**Narration Events:**
- `route_job` â€” Executing operation
- `infer_schedule` â€” Scheduling inference
- `infer_worker` â€” Worker selected
- `infer_token` â€” Each token generated
- `infer_complete` â€” Inference finished

---

### Step 5: Scheduler Selects Worker

**File:** `bin/15_queen_rbee_crates/scheduler/src/simple.rs`

```rust
async fn schedule(&self, request: JobRequest) -> Result<ScheduleResult, SchedulerError> {
    let job_id = &request.job_id;
    let model = &request.model;
    
    n!("infer_schedule", "ğŸ” Finding worker for model '{}'", model);
    
    // Find best worker for model
    let worker = self.worker_registry
        .find_best_worker_for_model(model)
        .ok_or_else(|| {
            n!("infer_no_worker", "âŒ No available worker found for model '{}'", model);
            SchedulerError::NoWorkersAvailable { model: model.clone() }
        })?;
    
    let worker_id = format!("{}-{}", worker.group, worker.instance);
    let worker_port: u16 = worker.instance.parse().unwrap_or(8080);
    
    n!("infer_worker_sel", "âœ… Selected worker '{}' for model '{}' at localhost:{}", 
        worker_id, model, worker_port);
    
    Ok(ScheduleResult {
        worker_id,
        worker_url: format!("http://localhost:{}", worker_port),
        worker_port,
        model: worker.model.clone().unwrap_or_else(|| model.to_string()),
        device: worker.group.clone(),
    })
}
```

**Location:** Lines 178-212  
**Purpose:** Find available worker for model

**Selection Criteria:**
1. Worker has model loaded
2. Worker is idle (gpu_util_pct == 0.0)
3. First match wins (simple scheduler)

---

### Step 6: Scheduler Executes on Worker

**File:** `bin/15_queen_rbee_crates/scheduler/src/simple.rs`

```rust
pub async fn execute_job<F>(
    &self,
    schedule_result: ScheduleResult,
    job_request: JobRequest,
    mut line_handler: F,
) -> Result<(), SchedulerError>
where
    F: FnMut(&str) -> Result<(), anyhow::Error>,
{
    let worker_url = &schedule_result.worker_url;
    
    // Build worker request
    let worker_request = WorkerInferenceRequest {
        prompt: job_request.prompt.clone(),
        max_tokens: job_request.max_tokens.unwrap_or(100),
        temperature: job_request.temperature.unwrap_or(0.7),
        top_p: job_request.top_p,
        top_k: job_request.top_k,
    };
    
    // POST to worker
    let client = reqwest::Client::new();
    let response = client
        .post(format!("{}/v1/jobs", worker_url))
        .json(&Operation::Infer(worker_request))
        .send()
        .await?;
    
    let worker_job: WorkerJobResponse = response.json().await?;
    
    // Connect to worker's SSE stream
    let stream_url = format!("{}{}", worker_url, worker_job.sse_url);
    let stream_response = client.get(&stream_url).send().await?;
    
    // Stream tokens back to client
    let mut stream = stream_response.bytes_stream();
    let mut buffer = String::new();
    
    while let Some(chunk) = stream.next().await {
        let chunk = chunk?;
        buffer.push_str(&String::from_utf8_lossy(&chunk));
        
        while let Some(newline_pos) = buffer.find('\n') {
            let line = buffer[..newline_pos].trim().to_string();
            buffer = buffer[newline_pos + 1..].to_string();
            
            if !line.is_empty() {
                let clean_line = if line.starts_with("data: ") { 
                    &line[6..] 
                } else { 
                    &line 
                };
                
                // Forward to client
                line_handler(clean_line)?;
                
                // Check for [DONE]
                if clean_line == "[DONE]" {
                    return Ok(());
                }
            }
        }
    }
    
    Ok(())
}
```

**Location:** Lines 60-172  
**Purpose:** Execute on worker and stream tokens

---

### Step 7: Worker Receives Request

**File:** `bin/30_llm_worker_rbee/src/job_router.rs`

```rust
pub async fn create_job(
    state: JobState,
    payload: serde_json::Value,
) -> Result<JobResponse> {
    // Parse operation
    let operation: Operation = serde_json::from_value(payload)?;
    
    let Operation::Infer(request) = operation else {
        return Err(anyhow::anyhow!("Unsupported operation"));
    };
    
    // Generate job ID
    let job_id = state.registry.create_job();
    
    // Create MPSC channel for tokens
    let (response_tx, response_rx) = tokio::sync::mpsc::unbounded_channel();
    
    // Store receiver for SSE streaming
    state.registry.store_receiver(&job_id, response_rx);
    
    // Create generation request
    let generation_request = GenerationRequest {
        request_id: job_id.clone(),
        prompt: request.prompt,
        config: GenerationConfig {
            max_tokens: request.max_tokens.unwrap_or(100),
            temperature: request.temperature.unwrap_or(0.7),
            top_p: request.top_p.unwrap_or(0.9),
            top_k: request.top_k,
        },
        response_tx,
    };
    
    // Add to queue
    state.queue.add_request(generation_request).await?;
    
    Ok(JobResponse {
        job_id: job_id.clone(),
        sse_url: format!("/v1/jobs/{}/stream", job_id),
    })
}
```

**Location:** Lines 51-90  
**Purpose:** Queue request for generation

---

### Step 8: Generation Engine Processes

**File:** `bin/30_llm_worker_rbee/src/backend/generation_engine.rs`

```rust
pub fn run(self) {
    std::thread::spawn(move || {
        let rt = tokio::runtime::Runtime::new().unwrap();
        
        loop {
            // Pull request from queue
            let request = if let Some(req) = rt.block_on(self.request_rx.recv()) {
                req
            } else {
                break;
            };
            
            // Lock backend (exclusive access)
            let mut backend = self.backend.lock().unwrap();
            
            // Generate tokens
            if let Err(e) = Self::generate_streaming(
                &mut backend,
                &request.prompt,
                &request.config,
                request.response_tx,
            ) {
                tracing::error!("Generation failed: {}", e);
            }
            
            // Lock released here
        }
    });
}

fn generate_streaming(
    backend: &mut CandleInferenceBackend,
    prompt: &str,
    config: &GenerationConfig,
    response_tx: UnboundedSender<TokenResponse>,
) -> Result<()> {
    // Tokenize prompt
    let tokens = backend.tokenizer.encode(prompt, true)?;
    
    // Reset KV cache
    backend.model.clear_kv_cache();
    
    // Generate tokens one by one
    for i in 0..config.max_tokens {
        let token = backend.generate_next_token(&tokens, config)?;
        let text = backend.tokenizer.decode(&[token], false)?;
        
        // Send token through channel
        response_tx.send(TokenResponse {
            token: text,
            token_count: i + 1,
        })?;
        
        // Check for EOS
        if token == backend.eos_token_id {
            break;
        }
    }
    
    Ok(())
}
```

**Location:** Lines 60-150  
**Purpose:** Generate tokens and send through channel

---

### Step 9: Worker Streams Tokens

**File:** `bin/30_llm_worker_rbee/src/http/stream.rs`

```rust
pub async fn handle_stream(
    Path(job_id): Path<String>,
    State(state): State<WorkerState>,
) -> Sse<impl Stream<Item = Result<Event, Infallible>>> {
    // Take receiver from registry
    let response_rx = state.registry.take_receiver(&job_id);
    
    let stream = async_stream::stream! {
        let Some(mut response_rx) = response_rx else {
            yield Ok(Event::default().data("ERROR: Job not found"));
            return;
        };
        
        let mut token_count = 0;
        
        // Stream tokens as they arrive
        while let Some(token_response) = response_rx.recv().await {
            token_count += 1;
            
            // Format as SSE event
            yield Ok(Event::default().json_data(&InferenceEvent::Token {
                t: token_response.token,
                i: token_count,
            }).unwrap());
        }
        
        // Send [DONE] marker
        yield Ok(Event::default().data("[DONE]"));
    };
    
    Sse::new(stream)
}
```

**Location:** Lines 100-140  
**Purpose:** Stream tokens to Queen via SSE

---

## Backpressure Handling

### MPSC Channel Backpressure

**Worker Side:**
```rust
// Unbounded channel - no backpressure
let (response_tx, response_rx) = tokio::sync::mpsc::unbounded_channel();

// Send never blocks
response_tx.send(token)?;
```

**Why Unbounded:**
- Generation is CPU-bound (slow)
- Network is fast (relative to generation)
- Tokens are small (~10 bytes each)
- Risk of blocking generation thread

**Alternative (Bounded):**
```rust
// Bounded channel with backpressure
let (response_tx, response_rx) = tokio::sync::mpsc::channel(100);

// Send blocks if channel full
response_tx.send(token).await?;
```

---

### SSE Stream Backpressure

**Queen Side:**
```rust
// SSE stream with automatic backpressure
let mut stream = response.bytes_stream();

while let Some(chunk) = stream.next().await {
    // Process chunk
    line_handler(clean_line)?;
}
```

**How It Works:**
- `bytes_stream()` uses HTTP/1.1 flow control
- If client slow, stream pauses
- Worker SSE stream blocks
- Generation continues (unbounded channel)

---

## Error Handling

### Worker Not Found

**Error:**
```
âŒ No available worker found for model 'tinyllama'
```

**Narration:** `infer_no_worker`  
**HTTP Status:** 500  
**Recovery:** Start worker with model

---

### Worker Timeout

**Error:**
```
â±ï¸  Worker request timed out after 30s
```

**Narration:** `infer_timeout`  
**HTTP Status:** 504  
**Recovery:** Retry or increase timeout

---

### Generation Error

**Error:**
```
âŒ Generation failed: Out of memory
```

**Narration:** `generation_error`  
**HTTP Status:** 500  
**Recovery:** Reduce max_tokens or use smaller model

---

### Connection Lost

**Error:**
```
âŒ Connection to worker lost
```

**Narration:** `connection_lost`  
**HTTP Status:** 502  
**Recovery:** Retry with different worker

---

## Narration Events Summary

### Queen Events

| Event | Message | Location |
|-------|---------|----------|
| `job_create` | "Job {job_id} created, waiting for client connection" | job_router.rs:64 |
| `route_job` | "Executing operation: {operation}" | job_router.rs:105 |
| `infer_schedule` | "ğŸ“‹ Scheduling inference to worker" | job_router.rs:175 |
| `infer_worker` | "âœ… Scheduled to worker: {worker_id}" | job_router.rs:180 |
| `infer_token` | "{token}" | job_router.rs:186 |
| `infer_complete` | "âœ… Inference complete" | job_router.rs:195 |

### Scheduler Events

| Event | Message | Location |
|-------|---------|----------|
| `infer_schedule` | "ğŸ” Finding worker for model '{model}'" | simple.rs:187 |
| `infer_no_worker` | "âŒ No available worker found for model '{model}'" | simple.rs:192 |
| `infer_worker_sel` | "âœ… Selected worker '{worker_id}' for model '{model}' at localhost:{port}" | simple.rs:202 |
| `infer_streaming` | "ğŸ“¡ Streaming tokens from worker..." | simple.rs:125 |
| `infer_complete` | "âœ… Inference complete" | simple.rs:154 |

### Worker Events

| Event | Message | Location |
|-------|---------|----------|
| `generation_start` | "Starting generation for job {job_id}" | generation_engine.rs:68 |
| `token_generated` | "Token {i}: {token}" | generation_engine.rs:140 |
| `generation_complete` | "Generation complete: {token_count} tokens" | generation_engine.rs:150 |

---

## Key Files Summary

| File | Purpose | Key Functions |
|------|---------|---------------|
| `bin/10_queen_rbee/src/job_router.rs` | Queen routing | `create_job()`, `execute_job()`, `route_operation()` |
| `bin/10_queen_rbee/src/http/jobs.rs` | Queen HTTP | `handle_create_job()`, `handle_stream_job()` |
| `bin/15_queen_rbee_crates/scheduler/src/simple.rs` | Scheduling | `schedule()`, `execute_job()` |
| `bin/30_llm_worker_rbee/src/job_router.rs` | Worker routing | `create_job()` |
| `bin/30_llm_worker_rbee/src/http/jobs.rs` | Worker HTTP | `handle_create_job()` |
| `bin/30_llm_worker_rbee/src/http/stream.rs` | Worker SSE | `handle_stream()` |
| `bin/30_llm_worker_rbee/src/backend/generation_engine.rs` | Token generation | `run()`, `generate_streaming()` |

---

## Performance Characteristics

### Latency Breakdown

- **Job creation:** <1ms
- **Worker selection:** <10ms
- **Worker HTTP POST:** ~5-10ms
- **First token:** 100-500ms (depends on model)
- **Token generation:** 10-50ms per token
- **SSE streaming:** <1ms per token
- **Total:** ~200ms + (tokens Ã— 20ms)

### Throughput

- **Single worker:** 20-100 tokens/second
- **Multiple workers:** Linear scaling
- **Bottleneck:** Model inference (CPU/GPU)

---

## Testing

### Manual Test

```bash
# Start queen
cargo run --bin queen-rbee -- --port 7833

# Start hive
cargo run --bin rbee-hive -- --port 7835

# Spawn worker
rbee-keeper worker spawn --model tinyllama --worker cpu --device 0

# Run inference
rbee-keeper infer --model tinyllama --prompt "Hello, world!"
```

### Expected Output

```
ğŸ“‹ Scheduling inference to worker
âœ… Scheduled to worker: worker-cpu-9123
Hello
,
 world
!
âœ… Inference complete
```

---

**Status:** âœ… COMPLETE  
**Total Documentation:** ~1,200 lines  
**All components documented with exact file paths and line numbers**
