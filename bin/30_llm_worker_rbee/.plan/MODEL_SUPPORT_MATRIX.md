# Model Support Matrix for rbee LLM Worker

**Created by:** TEAM-481  
**Date:** 2025-11-12

## Legend
- âœ… = Fully supported
- ğŸŸ¡ = Partially supported (one format only)
- âŒ = Not supported
- ğŸ” = Needs research
- ğŸ“ = Documentation only (already compatible)

---

## Current Support Matrix

| Model | Safetensors | GGUF | Candle Support | HF Downloads | Priority | Status |
|-------|-------------|------|----------------|--------------|----------|--------|
| **Llama** | âœ… | âœ… | âœ… | 17.8M+ | âœ… MVP | Complete |
| **Mistral** | âœ… | âœ… | âœ… | High | âœ… MVP | Complete |
| **Phi** | âœ… | âœ… | âœ… | High | âœ… MVP | Complete |
| **Qwen** | âœ… | âœ… | âœ… | 94.2M+ | âœ… MVP | Complete |
| **Gemma** | âŒ | âœ… | âœ… | High | ğŸ”¥ P1 | GGUF only |
| **DeepSeek** | âŒ | âŒ | âœ… | 421K+ | ğŸ”¥ P1 | Not implemented |
| **Mixtral** | âŒ | âŒ | âœ… | High | ğŸ”¥ P1 | Not implemented |
| **SmolLM** | ğŸ“ | ğŸ“ | âœ… (Llama) | 57.6K+ | ğŸ“ Doc | Already compatible |
| **Yi** | âŒ | âŒ | âœ… | 7.96K+ | ğŸ¯ P2 | Not implemented |
| **Starcoder2** | âŒ | âŒ | âœ… | Moderate | ğŸ¯ P2 | Not implemented |
| **Falcon** | âŒ | âŒ | âœ… | Moderate | ğŸ¯ P2 | Not implemented |
| **Stable-LM** | âŒ | âŒ | âœ… | Moderate | ğŸ¯ P2 | Not implemented |
| **Mamba** | âŒ | âŒ | âœ… | Low | ğŸ”® P3 | Not implemented |
| **RWKV** | âŒ | âŒ | âœ… | Low | ğŸ”® P3 | Not implemented |
| **Olmo** | âŒ | âŒ | âœ… | Moderate | ğŸ”® P3 | Not implemented |
| **Kimi** | âŒ | âŒ | ğŸ” | 277K+ | ğŸ” Research | Unknown architecture |
| **GPT-OSS** | âŒ | âŒ | ğŸ” | 4.76M+ | ğŸ” Research | Unknown architecture |
| **MiniMax-M2** | âŒ | âŒ | ğŸ” | 886K+ | ğŸ” Research | Unknown architecture |

---

## Detailed Model Information

### âœ… Fully Supported (5 families, 8 architectures)

#### 1. Llama Family
- **Variants:** Llama 2, Llama 3, Llama 3.1, Llama 3.2
- **Formats:** Safetensors âœ…, GGUF âœ…
- **Downloads:** 17.8M+ (Llama-3.1-8B-Instruct)
- **Files:**
  - `src/backend/models/llama.rs` (safetensors)
  - `src/backend/models/quantized_llama.rs` (GGUF)
- **Status:** âœ… Complete

#### 2. Mistral Family
- **Variants:** Mistral 7B, Mistral Instruct
- **Formats:** Safetensors âœ…, GGUF âœ…
- **Downloads:** High
- **Files:**
  - `src/backend/models/mistral.rs` (safetensors)
  - Uses `quantized_llama.rs` for GGUF (same format)
- **Status:** âœ… Complete

#### 3. Phi Family
- **Variants:** Phi-2, Phi-3
- **Formats:** Safetensors âœ…, GGUF âœ…
- **Downloads:** High
- **Files:**
  - `src/backend/models/phi.rs` (safetensors)
  - `src/backend/models/quantized_phi.rs` (GGUF)
- **Status:** âœ… Complete

#### 4. Qwen Family
- **Variants:** Qwen2, Qwen2.5
- **Formats:** Safetensors âœ…, GGUF âœ…
- **Downloads:** 94.2M+ (Qwen2.5-1.5B-Instruct)
- **Files:**
  - `src/backend/models/qwen.rs` (safetensors)
  - `src/backend/models/quantized_qwen.rs` (GGUF)
- **Status:** âœ… Complete

#### 5. Gemma Family (Partial)
- **Variants:** Gemma, Gemma 2, Gemma 3
- **Formats:** Safetensors âŒ, GGUF âœ…
- **Downloads:** High (Google)
- **Files:**
  - `src/backend/models/quantized_gemma.rs` (GGUF only)
- **Status:** ğŸŸ¡ GGUF only - **needs safetensors support**

---

### ğŸ”¥ Priority 1: MVP Critical (Implement Next)

#### 6. DeepSeek Family
- **Variants:** DeepSeek-R1, DeepSeek-V2
- **Formats:** Safetensors âŒ, GGUF âŒ
- **Downloads:** 421K+ (DeepSeek-R1) - **Trending #1 on HuggingFace**
- **Candle Support:** âœ… YES (`candle-transformers/src/models/deepseek2.rs`)
- **Effort:** MEDIUM (2-3 days)
- **Files to create:**
  - `src/backend/models/deepseek.rs` (safetensors)
  - `src/backend/models/quantized_deepseek.rs` (GGUF)
- **Status:** âŒ Not implemented - **HIGHEST PRIORITY**

#### 7. Gemma (Safetensors)
- **Action:** Complete existing GGUF support
- **Formats:** Safetensors âŒ, GGUF âœ…
- **Candle Support:** âœ… YES (`candle-transformers/src/models/gemma.rs`)
- **Effort:** LOW (1-2 days)
- **Files to create:**
  - `src/backend/models/gemma.rs` (safetensors)
- **Status:** ğŸŸ¡ GGUF only - **needs safetensors**

#### 8. Mixtral (MoE)
- **Variants:** Mixtral-8x7B
- **Formats:** Safetensors âŒ, GGUF âŒ
- **Downloads:** High (Mistral AI)
- **Candle Support:** âœ… YES (`candle-transformers/src/models/mixtral.rs`)
- **Effort:** MEDIUM (2-3 days)
- **Files to create:**
  - `src/backend/models/mixtral.rs` (safetensors)
  - `src/backend/models/quantized_mixtral.rs` (GGUF)
- **Status:** âŒ Not implemented - **MoE differentiator**

---

### ğŸ¯ Priority 2: Post-MVP

#### 9. Yi Family
- **Variants:** Yi-6B, Yi-34B
- **Formats:** Safetensors âŒ, GGUF âŒ
- **Downloads:** 7.96K+
- **Candle Support:** âœ… YES (`candle-transformers/src/models/yi.rs`)
- **Effort:** MEDIUM (2-3 days)
- **Status:** âŒ Not implemented

#### 10. Starcoder2 Family
- **Variants:** Starcoder2-3B, Starcoder2-7B, Starcoder2-15B
- **Formats:** Safetensors âŒ, GGUF âŒ
- **Downloads:** Moderate (code generation specialist)
- **Candle Support:** âœ… YES (`candle-transformers/src/models/starcoder2.rs`)
- **Effort:** MEDIUM (2-3 days)
- **Status:** âŒ Not implemented

#### 11. Falcon Family
- **Variants:** Falcon-7B, Falcon-40B
- **Formats:** Safetensors âŒ, GGUF âŒ
- **Downloads:** Moderate
- **Candle Support:** âœ… YES (`candle-transformers/src/models/falcon.rs`)
- **Effort:** MEDIUM (2-3 days)
- **Status:** âŒ Not implemented

#### 12. Stable-LM Family
- **Variants:** Stable-LM-3B, Stable-LM-7B
- **Formats:** Safetensors âŒ, GGUF âŒ
- **Downloads:** Moderate (Stability AI)
- **Candle Support:** âœ… YES (`candle-transformers/src/models/stable_lm.rs`)
- **Effort:** MEDIUM (2-3 days)
- **Status:** âŒ Not implemented

---

### ğŸ”® Priority 3: Future/Experimental

#### 13. Mamba Family
- **Variants:** Mamba-130M, Mamba-370M, Mamba-790M, Mamba-1.4B, Mamba-2.8B
- **Formats:** Safetensors âŒ, GGUF âŒ
- **Downloads:** Low (experimental)
- **Architecture:** State-space models (alternative to transformers)
- **Candle Support:** âœ… YES (`candle-transformers/src/models/mamba.rs`)
- **Effort:** HIGH (different architecture)
- **Status:** âŒ Not implemented

#### 14. RWKV Family
- **Variants:** RWKV-v5, RWKV-v6
- **Formats:** Safetensors âŒ, GGUF âŒ
- **Downloads:** Low (niche)
- **Architecture:** RNN-based alternative to transformers
- **Candle Support:** âœ… YES (`candle-transformers/src/models/rwkv_v5.rs`, `rwkv_v6.rs`)
- **Effort:** HIGH (different architecture)
- **Status:** âŒ Not implemented

#### 15. Olmo Family
- **Variants:** Olmo-1B, Olmo-7B, Olmo2
- **Formats:** Safetensors âŒ, GGUF âŒ
- **Downloads:** Moderate (Allen Institute)
- **Candle Support:** âœ… YES (`candle-transformers/src/models/olmo.rs`, `olmo2.rs`)
- **Effort:** MEDIUM (2-3 days)
- **Status:** âŒ Not implemented

---

### ğŸ” Needs Research (Unknown Architecture)

#### 16. Kimi Family (Moonshot AI)
- **Variants:** Kimi-K2-Thinking, Kimi-Linear-48B, Kimi-K2-Instruct
- **Formats:** Unknown
- **Downloads:** 89.5K+ (Kimi-K2-Thinking), 277K+ (Kimi-Linear-48B)
- **Architecture:** Unknown (possibly Llama-based?)
- **Candle Support:** ğŸ” UNKNOWN - needs research
- **Effort:** HIGH (architecture unknown)
- **Status:** ğŸ” Needs research

#### 17. GPT-OSS (OpenAI)
- **Variants:** GPT-OSS-20B
- **Formats:** Unknown
- **Downloads:** 4.76M+
- **Architecture:** Unknown
- **Candle Support:** ğŸ” UNKNOWN - needs research
- **Effort:** HIGH (architecture unknown)
- **Status:** ğŸ” Needs research

#### 18. MiniMaxAI/MiniMax-M2
- **Variants:** MiniMax-M2
- **Formats:** Unknown
- **Downloads:** 886K+
- **Architecture:** Unknown
- **Candle Support:** ğŸ” UNKNOWN - needs research
- **Effort:** HIGH (architecture unknown)
- **Status:** ğŸ” Needs research

---

### ğŸ“ Already Compatible (Documentation Only)

#### 19. SmolLM Family
- **Variants:** SmolLM-135M, SmolLM-360M, SmolLM-1.7B, SmolLM2, SmolLM3-3B
- **Formats:** Uses Llama architecture
- **Downloads:** 57.6K+ (SmolLM3-3B)
- **Architecture:** Llama-based
- **Candle Support:** âœ… YES (uses Llama loader)
- **Effort:** NONE (just documentation)
- **Status:** ğŸ“ Already compatible via Llama - **just document**

---

## Summary Statistics

### Current Support
- **Total Families:** 5 (Llama, Mistral, Phi, Qwen, Gemma)
- **Total Architectures:** 8 (including quantized variants)
- **Safetensors Support:** 4/5 families (80%)
- **GGUF Support:** 5/5 families (100%)
- **Coverage:** ~60% of popular HuggingFace models

### After MVP (Priority 1)
- **Total Families:** 7 (+ DeepSeek, Mixtral)
- **Total Architectures:** 12 (including quantized variants)
- **Safetensors Support:** 7/7 families (100%)
- **GGUF Support:** 7/7 families (100%)
- **Coverage:** ~90% of popular HuggingFace models

### After Priority 2
- **Total Families:** 11 (+ Yi, Starcoder2, Falcon, Stable-LM)
- **Total Architectures:** 18 (including quantized variants)
- **Coverage:** ~95% of popular HuggingFace models

---

## Implementation Effort Summary

| Priority | Models | Total Effort | Expected Impact |
|----------|--------|--------------|-----------------|
| **P1 (MVP)** | DeepSeek, Gemma (safetensors), Mixtral | 1-2 weeks | 90% coverage |
| **P2 (Post-MVP)** | Yi, Starcoder2, Falcon, Stable-LM | 2-3 weeks | 95% coverage |
| **P3 (Future)** | Mamba, RWKV, Olmo | 3-4 weeks | 98% coverage |
| **Research** | Kimi, GPT-OSS, MiniMax-M2 | Unknown | Unknown |

---

## Next Actions

1. âœ… **Approve this plan**
2. ğŸ”¥ **TEAM-482:** Implement DeepSeek-R1 (Priority 1, highest impact)
3. ğŸ”¥ **TEAM-483:** Add Gemma safetensors support (Priority 1, low effort)
4. ğŸ”¥ **TEAM-484:** Implement Mixtral MoE (Priority 1, differentiator)
5. ğŸ“ **TEAM-485:** Document SmolLM compatibility (already works)

---

**Status:** âœ… PLANNING COMPLETE  
**Last Updated:** 2025-11-12  
**Next Review:** After MVP implementation
