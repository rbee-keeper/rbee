# rbee: Technical Differentiators

**Audience:** Technical decision makers, CTOs, architects  
**Date:** November 2, 2025

---

## Overview

This document compares rbee against alternative solutions across consumer and business use cases.

---

## Consumer Use Case Comparisons

### vs ComfyUI + Ollama + Whisper (Current Multi-Tool Setup)

| Feature | rbee | ComfyUI + Ollama + Whisper |
|---------|------|---------------------------|
| **Setup Time** | 5 minutes | Hours (per tool) |
| **One API** | âœ… Port 7833 | âŒ 3+ different ports |
| **Multi-GPU** | âœ… Automatic orchestration | âŒ Manual, conflicts |
| **Heterogeneous** | âœ… CUDA + Metal + CPU | âŒ Per-tool configuration |
| **OpenAI Compatible** | âœ… Drop-in replacement | âŒ Custom APIs |
| **GUI Management** | âœ… Unified dashboard | âŒ 3 different UIs |
| **Scripting** | âœ… Rhai for routing | âŒ None |
| **No Conflicts** | âœ… Automatic allocation | âŒ Fight over GPU memory |
| **Cost** | Free (GPL) | Free (various licenses) |

**Winner: rbee** - One API, no conflicts, automatic orchestration

---

### vs Ollama Alone

| Feature | rbee | Ollama |
|---------|------|--------|
| **Multi-Machine** | âœ… SSH-based orchestration | âŒ Single machine |
| **Multi-Modal** | âœ… Text, images, audio, video | âŒ Text only |
| **Custom Routing** | âœ… Rhai scripts | âŒ None |
| **GPU Pinning** | âœ… GUI + scripts | âŒ Manual env vars |
| **OpenAI Compatible** | âœ… Full compatibility | âœ… Basic compatibility |
| **Simplicity** | Medium (more features) | âœ… Very simple |
| **Maturity** | ğŸš§ 68% complete | âœ… Battle-tested |

**Trade-off:**
- **Ollama wins on:** Simplicity, maturity
- **rbee wins on:** Multi-machine, multi-modal, custom routing

**Use Ollama if:** You want simple single-machine LLM inference  
**Use rbee if:** You want to orchestrate multiple GPUs across multiple machines

---

### vs LocalAI

| Feature | rbee | LocalAI |
|---------|------|---------|
| **Multi-Machine** | âœ… Native support | âŒ Single machine |
| **Custom Routing** | âœ… Rhai scripts | âŒ None |
| **GPU Pinning** | âœ… GUI + scripts | âŒ Manual |
| **Heterogeneous** | âœ… CUDA + Metal + CPU | âš ï¸ Limited |
| **OpenAI Compatible** | âœ… Full compatibility | âœ… Full compatibility |
| **Multi-Modal** | âœ… (M3) | âœ… |
| **Maturity** | ğŸš§ 68% complete | âœ… Mature |

**Trade-off:**
- **LocalAI wins on:** Maturity, current multi-modal support
- **rbee wins on:** Multi-machine orchestration, custom routing

**Use LocalAI if:** You want mature single-machine multi-modal inference  
**Use rbee if:** You want to orchestrate multiple machines with custom routing

---

## Business Use Case Comparisons

### vs Building from Scratch (vLLM + ComfyUI + Whisper)

| Aspect | rbee | Build from Scratch |
|--------|------|-------------------|
| **Development Time** | 1 day | 6-12 months |
| **Development Cost** | ~$500 | $450K-1M |
| **Engineers Needed** | 1 (setup) | 3-5 (development) |
| **Multi-Tenancy** | âœ… Rhai scripts | Custom code (3 months) |
| **GDPR Compliance** | âœ… Built-in | Custom code (6 months) |
| **Quota Enforcement** | âœ… Rhai scripts | Custom code (2 months) |
| **Audit Logging** | âœ… Built-in | Custom code (3 months) |
| **Monitoring** | âœ… Prometheus + Grafana | Custom (2 months) |
| **API Gateway** | âœ… Built-in | Custom (2 months) |
| **Load Balancing** | âœ… Rhai scripts | Custom (2 months) |
| **Ongoing Maintenance** | Community updates | 2 engineers |

**ROI Analysis:**

**Build from Scratch:**
- Year 1: $450K-1M (development) + $300K-400K (maintenance) = **$750K-1.4M**
- Year 2+: $300K-400K/year (maintenance)

**rbee (Self-Hosted):**
- Year 1: $500 (setup) + $0 (maintenance) = **$500**
- Year 2+: $0/year (community updates)

**Savings: $750K-1.4M in year 1**

---

### vs Ray + KServe (Kubernetes-Based)

| Feature | rbee | Ray + KServe |
|---------|------|--------------|
| **Setup Complexity** | Low (SSH-based) | High (Kubernetes) |
| **Infrastructure** | Any Linux machines | Kubernetes cluster |
| **Learning Curve** | Low (Rhai scripts) | High (K8s + Ray) |
| **Multi-Tenancy** | âœ… Rhai scripts | âœ… K8s namespaces |
| **Resource Isolation** | Process-based | Container-based |
| **Overhead** | Low (~1ms routing) | Medium (K8s overhead) |
| **Homelab Friendly** | âœ… SSH-based | âŒ Requires K8s |
| **Enterprise Features** | GDPR, audit logs | RBAC, namespaces |
| **Maturity** | ğŸš§ 68% complete | âœ… Battle-tested |

**Trade-off:**
- **Ray + KServe wins on:** Enterprise features, ecosystem integration, maturity
- **rbee wins on:** Simplicity, homelab-friendly, lower overhead

**Use Ray + KServe if:** You have Kubernetes expertise and need enterprise RBAC  
**Use rbee if:** You want simple SSH-based deployment without K8s complexity

---

### vs Together.ai / Replicate (Managed Platforms)

| Aspect | rbee (Self-Hosted) | Together.ai / Replicate |
|--------|-------------------|------------------------|
| **Control** | âœ… Full control | âŒ Provider-controlled |
| **Custom Models** | âœ… Any model | âš ï¸ Limited selection |
| **Data Privacy** | âœ… Your infrastructure | âŒ Shared infrastructure |
| **GDPR Compliance** | âœ… Built-in, your control | âš ï¸ Provider-dependent |
| **Cost** | GPU + electricity | Per-token pricing |
| **Margins** | 100% (self-hosted) | 50-70% (after fees) |
| **Setup Time** | 1 day | 1 hour |
| **Maintenance** | You manage | Provider manages |
| **SLA** | You define | Provider defines |

**Cost Comparison (1M tokens/day):**

**Together.ai:**
- Cost: $0.20/1M tokens Ã— 30M tokens/month = $6,000/month
- Your revenue: $10,000/month (example)
- **Profit: $4,000/month**

**rbee (Self-Hosted):**
- Cost: GPU electricity ~$2,000/month
- Your revenue: $10,000/month (example)
- **Profit: $8,000/month**

**Extra profit with rbee: $4,000/month = $48K/year**

---

### vs OpenAI / Anthropic (Cloud APIs)

| Aspect | rbee (Business) | OpenAI / Anthropic |
|--------|----------------|-------------------|
| **Use Case** | You offer AI services | You consume AI services |
| **Target** | Your customers | You |
| **Revenue** | You earn | You pay |
| **Control** | âœ… Full control | âŒ Provider-controlled |
| **Custom Models** | âœ… Any model | âŒ Provider models only |
| **Pricing** | You set | Provider sets |
| **Data Privacy** | âœ… Your infrastructure | âŒ Shared infrastructure |
| **GDPR** | âœ… Your control | âš ï¸ Provider-dependent |

**Not a direct comparison** - Different use cases (provider vs consumer)

---

## Technical Architecture Comparisons

### Smart/Dumb Architecture

**rbee's approach:**
```
queen-rbee (THE BRAIN)
  â†“ Makes ALL decisions
  â†“ Routes to workers
  
llm-worker-rbee (EXECUTOR)
  â†“ Loads model
  â†“ Executes inference
  â†“ Streams tokens
```

**Benefits:**
- âœ… Easy to debug (one place for logic)
- âœ… Easy to customize (Rhai scripts, no recompilation)
- âœ… Easy to test (executors are deterministic)
- âœ… Scalable (add workers without queen changes)

**Alternative (vLLM approach):**
```
vLLM Server (Monolithic)
  â†“ Manages models
  â†“ Schedules requests
  â†“ Executes inference
  â†“ Streams tokens
```

**Trade-offs:**
- âœ… Simpler (one binary)
- âŒ Harder to customize (recompilation needed)
- âŒ Harder to scale (monolithic)

---

### Process Isolation

**rbee's approach:**
```
Process 1: llm-worker-rbee (GPU 0)
  â†“ CUDA context 0
  â†“ VRAM isolated

Process 2: llm-worker-rbee (GPU 1)
  â†“ CUDA context 1
  â†“ VRAM isolated
```

**Benefits:**
- âœ… No memory corruption
- âœ… Clean VRAM lifecycle
- âœ… Kill safety (kill worker = clean VRAM)
- âœ… Standalone testing

**Alternative (Ollama approach):**
```
Single Process: ollama
  â†“ Manages all models
  â†“ Shares CUDA context
```

**Trade-offs:**
- âœ… Lower overhead (one process)
- âŒ Shared memory (potential corruption)
- âŒ Kill = lose all models

---

### Job-Based Architecture

**rbee's approach:**
```
POST /v1/jobs â†’ job_id
GET /v1/jobs/{job_id}/stream â†’ SSE events
```

**Benefits:**
- âœ… Real-time feedback (SSE streaming)
- âœ… Job isolation (separate streams)
- âœ… Audit trail (every job logged)
- âœ… Cancellation support

**Alternative (OpenAI approach):**
```
POST /v1/chat/completions â†’ SSE stream
```

**Trade-offs:**
- âœ… Simpler (one endpoint)
- âŒ No job tracking
- âŒ No audit trail
- âŒ No cancellation

---

## Feature Matrix

### Consumer Features

| Feature | rbee | Ollama | LocalAI | ComfyUI + Ollama |
|---------|------|--------|---------|-----------------|
| **Multi-Machine** | âœ… | âŒ | âŒ | âŒ |
| **Multi-Modal** | âœ… (M3) | âŒ | âœ… | âœ… (separate) |
| **Custom Routing** | âœ… Rhai | âŒ | âŒ | âŒ |
| **GPU Pinning** | âœ… GUI | âŒ | âŒ | âŒ |
| **OpenAI Compatible** | âœ… | âœ… | âœ… | âŒ |
| **No Conflicts** | âœ… | âš ï¸ | âš ï¸ | âŒ |
| **Unified Dashboard** | âœ… | âŒ | âœ… | âŒ |
| **Free** | âœ… GPL | âœ… MIT | âœ… MIT | âœ… Various |

---

### Business Features

| Feature | rbee | Build from Scratch | Ray + KServe | Together.ai |
|---------|------|-------------------|--------------|-------------|
| **Multi-Tenancy** | âœ… Rhai | âœ… Custom | âœ… K8s | âœ… |
| **GDPR Compliance** | âœ… Built-in | âœ… Custom | âš ï¸ Manual | âš ï¸ Provider |
| **Quota Enforcement** | âœ… Rhai | âœ… Custom | âœ… K8s | âœ… |
| **Audit Logging** | âœ… Built-in | âœ… Custom | âš ï¸ Manual | âš ï¸ Provider |
| **Custom Models** | âœ… | âœ… | âœ… | âš ï¸ Limited |
| **Setup Time** | 1 day | 6-12 months | 1 week | 1 hour |
| **Cost** | GPU only | $500K+ | GPU + K8s | Per-token |
| **Control** | âœ… Full | âœ… Full | âœ… Full | âŒ Limited |

---

## When to Choose rbee

### Choose rbee if:

**Consumer:**
- âœ… You have multiple computers with GPUs
- âœ… You want to use them all together
- âœ… You're tired of juggling multiple AI tools
- âœ… You want OpenAI compatibility
- âœ… You want custom routing (Rhai scripts)

**Business:**
- âœ… You have GPU infrastructure
- âœ… You want to offer AI services
- âœ… You need multi-tenancy
- âœ… You need GDPR compliance
- âœ… You want to avoid 6-12 months of development
- âœ… You want to keep 100% of revenue (self-hosted)

---

### Don't choose rbee if:

**Consumer:**
- âŒ You only have one computer with one GPU â†’ Use Ollama
- âŒ You want maximum simplicity â†’ Use Ollama
- âŒ You need battle-tested maturity â†’ Use Ollama or LocalAI

**Business:**
- âŒ You have Kubernetes expertise and need enterprise RBAC â†’ Use Ray + KServe
- âŒ You want zero infrastructure management â†’ Use Together.ai or Replicate
- âŒ You need production-ready NOW â†’ Wait for rbee M1 (Q1 2026) or use alternatives

---

## Unique rbee Advantages

### 1. Heterogeneous Hardware Support

**rbee is the ONLY solution that natively supports:**
- NVIDIA CUDA (Windows/Linux)
- Apple Metal (Mac M1/M2/M3)
- CPU fallback (any machine)
- **All in one cluster**

**Example:**
```
Gaming PC (RTX 4090) + Mac Studio (M2 Ultra) + Old Server (CPU)
= One unified API
```

**No other solution does this.**

---

### 2. Rhai Programmable Scheduler

**rbee is the ONLY solution with:**
- User-scriptable routing (no recompilation)
- Multi-tenancy via scripts
- Quota enforcement via scripts
- Cost optimization via scripts

**Example:**
```rhai
fn route_task(task, workers) {
    if task.customer_tier == "enterprise" {
        return workers.filter(|w| w.gpu_type == "H100").least_loaded();
    }
    return workers.filter(|w| w.gpu_type == "A100").least_loaded();
}
```

**No other solution offers this level of customization without code changes.**

---

### 3. GDPR Compliance Out of the Box

**rbee is the ONLY solution with:**
- Immutable audit logs (7-year retention)
- Data export endpoints (`/gdpr/export`)
- Data deletion endpoints (`/gdpr/delete`)
- EU-only worker filtering
- Consent tracking

**No other open-source solution has this.**

---

### 4. SSH-Based Deployment (Homelab-Friendly)

**rbee is the ONLY solution that:**
- Installs via SSH (like Ansible)
- No Kubernetes required
- No Docker required (optional)
- Works on any Linux machine

**Perfect for homelabs.**

---

## Summary

| Use Case | Best Choice | Why |
|----------|-------------|-----|
| **Single machine, simple LLM** | Ollama | Simplicity, maturity |
| **Single machine, multi-modal** | LocalAI | Maturity, current support |
| **Multi-machine, homelab** | **rbee** | Only solution for this |
| **Business, simple setup** | **rbee** | 1 day vs 6-12 months |
| **Business, Kubernetes** | Ray + KServe | Enterprise features |
| **Business, zero infra** | Together.ai | Managed platform |

---

**rbee's sweet spot:** Multi-machine GPU orchestration with custom routing for consumers and businesses.

---

## Next Steps

1. **Evaluate your needs:** Consumer or business?
2. **Compare alternatives:** Use this document
3. **Try rbee:** See [README.md](../../README.md)
4. **Read use cases:** [Consumer](02_CONSUMER_USE_CASE.md) or [Business](03_BUSINESS_USE_CASE.md)
